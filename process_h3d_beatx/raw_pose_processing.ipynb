{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Poses from Amass Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import rotation_conversions as rc\n",
    "\n",
    "\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c\n",
    "\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the device to run the body model on.\n",
    "comp_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smplx\n",
    "smplx_bm = smplx.create(\n",
    "            \"../datasets/hub/smplx_models/\", \n",
    "            model_type='smplx',\n",
    "            gender='NEUTRAL_2020', \n",
    "            use_face_contour=False,\n",
    "            num_betas=300,\n",
    "            num_expression_coeffs=100, \n",
    "            ext='npz',\n",
    "            use_pca=False,\n",
    "        ).cuda().eval()\n",
    "\n",
    "def load_amass(data):\n",
    "    ## 这个是用来\n",
    "    # 修改amass数据里面的朝向，原本在blender里面是Z轴向上，目标是Y轴向上，当时面向目前没改\n",
    "    \n",
    "    data_dict = {key: data[key] for key in data}\n",
    "    frames = data_dict['poses'].shape[0]\n",
    "    # b = data_dict['poses'][...,:3]\n",
    "    # b = rc.axis_angle_to_matrix(torch.from_numpy(b))\n",
    "    # rot_matrix = np.array([[1.0, 0.0, 0.0], [0.0 , 0.0, 1.0], [0.0, -1.0, 0.0]])\n",
    "    # c = np.einsum('ij,kjl->kil',rot_matrix,b)\n",
    "    # c = rc.matrix_to_axis_angle(torch.from_numpy(c))\n",
    "    # data_dict['poses'][...,:3] = c\n",
    "    \n",
    "    # trans_matrix1 = np.array([[1.0, 0.0, 0.0], [0.0 , 0.0, -1.0], [0.0, 1.0, 0.0]])\n",
    "    # data_dict['trans'] = np.einsum(\"bi,ij->bj\",data_dict['trans'],trans_matrix1)\n",
    "    \n",
    "    betas300 = np.zeros(300)\n",
    "    betas300[:16] = data_dict['betas']\n",
    "    data_dict['betas'] = betas300\n",
    "    data_dict[\"expressions\"] = np.zeros((frames,100))\n",
    "    return data_dict\n",
    "\n",
    "def load_beat(data):\n",
    "    \n",
    "    data_dict = {key: data[key] for key in data}\n",
    "    frames = data_dict['poses'].shape[0]\n",
    "\n",
    "    b = data_dict['poses'][...,:3]\n",
    "    b = rc.axis_angle_to_matrix(torch.from_numpy(b))\n",
    "    rot_matrix = np.array([[1.0, 0.0, 0.0], [0.0 , 0.0, -1.0], [0.0, 1.0, 0.0]])\n",
    "    \n",
    "\n",
    "    c = np.einsum('ij,kjl->kil',rot_matrix,b)\n",
    "    \n",
    "    \n",
    "    c = rc.matrix_to_axis_angle(torch.from_numpy(c))\n",
    "    data_dict['poses'][...,:3] = c\n",
    "    \n",
    "    trans_matrix1 = np.array([[1.0, 0.0, 0.0], [0.0 , 0.0, 1.0], [0.0, -1.0, 0.0]])\n",
    "    \n",
    "    #trans_matrix2 = np.array([[-1.0, 0.0, 0.0], [0.0 , -1.0, 0.0], [0.0, 0.0, 1.0]])\n",
    "    #trans_matrix2 = np.array([[1.0, 0.0, 0.0], [0.0 , -1.0, 0.0], [0.0, 0.0, -1.0]])\n",
    "    #data_dict['trans'] = np.einsum(\"bi,ij->bj\",data_dict['trans'],trans_matrix)\n",
    "    data_dict['trans'] = np.einsum(\"bi,ij->bj\",data_dict['trans'],trans_matrix1)\n",
    "\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "folders = []\n",
    "dataset_names = []\n",
    "for root, dirs, files in os.walk('./beat_data'):\n",
    "#     print(root, dirs, files)\n",
    "#     for folder in dirs:\n",
    "#         folders.append(os.path.join(root, folder))\n",
    "    folders.append(root)\n",
    "    for name in files:\n",
    "        paths.append(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root = './pose_data'\n",
    "save_folders = [folder.replace('./beat_data', './pose_data') for folder in folders]\n",
    "for folder in save_folders:\n",
    "    os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_matrix = np.array([[1.0, 0.0, 0.0],\n",
    "                            [0.0, 0.0, 1.0],\n",
    "                            [0.0, 1.0, 0.0]])\n",
    "ex_fps = 30\n",
    "target_joint = list(range(22))+list(range(25,55))\n",
    "def amass_to_pose(src_path, save_path):\n",
    "    bdata = np.load(src_path, allow_pickle=True)\n",
    "    if len(bdata.files)==6:\n",
    "        #print(f\"# ---- state file ---- #\")\n",
    "        return 0\n",
    "    bdata = load_beat(bdata)\n",
    "    fps = 0\n",
    "    try:\n",
    "        fps = bdata['mocap_frame_rate']\n",
    "        frame_number = bdata['trans'].shape[0]\n",
    "    except:\n",
    "#         print(list(bdata.keys()))\n",
    "        return fps\n",
    "    fId = 0 # frame id of the mocap sequence\n",
    "    pose_seq = []    \n",
    "    \n",
    "    betas = bdata['betas'].reshape(1,300)\n",
    "    betas = np.tile(betas, (frame_number, 1))\n",
    "    poses = bdata['poses']\n",
    "    trans = bdata['trans']\n",
    "    exps = bdata['expressions']\n",
    "    \n",
    "\n",
    "     \n",
    "    down_sample = int(fps / ex_fps)\n",
    "#     print(frame_number)\n",
    "#     print(fps)\n",
    "    betas = torch.from_numpy(betas).float().to(comp_device)[::down_sample]\n",
    "    poses = torch.from_numpy(poses).float().to(comp_device)[::down_sample]\n",
    "    trans = torch.from_numpy(trans).float().to(comp_device)[::down_sample]\n",
    "    exps = torch.from_numpy(exps).float().to(comp_device)[::down_sample]\n",
    "    \n",
    "    \n",
    "    n, c = poses.shape[0], poses.shape[1]\n",
    "    max_length = 128\n",
    "    s, r = n//max_length, n%max_length\n",
    "    #print(n, s, r)\n",
    "    all_tensor = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(s):\n",
    "            with torch.no_grad():\n",
    "                joints = smplx_bm(\n",
    "                    betas=betas[i*max_length:(i+1)*max_length], \n",
    "                    transl=trans[i*max_length:(i+1)*max_length], \n",
    "                    expression=exps[i*max_length:(i+1)*max_length], \n",
    "                    jaw_pose=poses[i*max_length:(i+1)*max_length, 66:69], \n",
    "                    global_orient=poses[i*max_length:(i+1)*max_length,:3], \n",
    "                    body_pose=poses[i*max_length:(i+1)*max_length,3:21*3+3], \n",
    "                    left_hand_pose=poses[i*max_length:(i+1)*max_length,25*3:40*3], \n",
    "                    right_hand_pose=poses[i*max_length:(i+1)*max_length,40*3:55*3], \n",
    "                    return_verts=True,\n",
    "                    return_joints=True,\n",
    "                    leye_pose=poses[i*max_length:(i+1)*max_length, 69:72], \n",
    "                    reye_pose=poses[i*max_length:(i+1)*max_length, 72:75],\n",
    "                )['joints'][:, target_joint, :]\n",
    "            pose_seq.append(joints)\n",
    "        if r != 0:\n",
    "            with torch.no_grad():\n",
    "                joints = smplx_bm(\n",
    "                    betas=betas[s*max_length:s*max_length+r], \n",
    "                    transl=trans[s*max_length:s*max_length+r], \n",
    "                    expression=exps[s*max_length:s*max_length+r], \n",
    "                    jaw_pose=poses[s*max_length:s*max_length+r, 66:69], \n",
    "                    global_orient=poses[s*max_length:s*max_length+r,:3], \n",
    "                    body_pose=poses[s*max_length:s*max_length+r,3:21*3+3], \n",
    "                    left_hand_pose=poses[s*max_length:s*max_length+r,25*3:40*3], \n",
    "                    right_hand_pose=poses[s*max_length:s*max_length+r,40*3:55*3], \n",
    "                    return_verts=True,\n",
    "                    return_joints=True,\n",
    "                    leye_pose=poses[s*max_length:s*max_length+r, 69:72], \n",
    "                    reye_pose=poses[s*max_length:s*max_length+r, 72:75],\n",
    "                )['joints'][:, target_joint, :]\n",
    "            pose_seq.append(joints)\n",
    "\n",
    "    pose_seq = torch.cat(pose_seq, dim=0)\n",
    "    \n",
    "    pose_seq_np = pose_seq.detach().cpu().numpy()\n",
    "    pose_seq_np_n = np.dot(pose_seq_np, trans_matrix)\n",
    "    pose_seq_np_n[..., 0] *= -1\n",
    "    np.save(save_path, pose_seq_np_n)\n",
    "    return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#all_count = sum([len(paths) for paths in group_path])\n",
    "cur_count = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a few hours for all datasets, here we take one dataset as an example\n",
    "\n",
    "To accelerate the process, you could run multiple scripts like this at one time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "pbar = tqdm(paths)\n",
    "pbar.set_description('Processing:beat')\n",
    "for path in pbar:\n",
    "    fps = 0\n",
    "    save_path = path.replace('./beat_data', './pose_data')\n",
    "    save_path = save_path[:-3] + 'npy'\n",
    "    fps = amass_to_pose(path, save_path)\n",
    "    cur_count += len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will extract poses from **BEATX** dataset, and put them under directory **\"./pose_data\"**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
